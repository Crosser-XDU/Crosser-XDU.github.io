## 引言
之前看完一些在线优化的论文后一直没有写一些笔记，有一些想法都没有记录下来。最近看完了大佬Shai Shalev-Shwartz写的对在线学习的一篇survey，对之前学的算法有了更清晰的理解（不得不说大佬的文章写的是真的好，算法讲的通俗易懂），就想着写一个专栏来整理出一条自己学习的在线学习算法的路线。

## Motivation
学习一个算法，首先要知道这个算法提出的动机是什么，这个算法解决了什么以前的算法没有解决的问题。那么在线学习是为了解决什么问题呢？

同在线学习不同，传统的机器学习模式叫做离线学习。举传统的分类任务为例，我们知道基本的学习流程是这样的：我们首先收集出一个含有大量数据的数据集，然后设计一个模型，参数随机初始化，通过在这个数据集上训练不断更新参数，让模型在数据集上的预测损失越来越小，等到在这个数据集上训练完成之后，最后将这个模型用于分类任务中去。我们可以看到，这个过程是封闭静态的（封闭是指只利用数据集的数据进行训练，静态的是指在训练之后模型就保持不动了）。并且在训练的过程中，我们假设数据集中的数据都是独立同分布的，算法本质上是学习到这个分布。但在实际的情况中，我们现实的环境是多变的，任务中看到的数据未必就是同分布的，我们就希望能利用最新的数据来更新模型，让模型一直在学习，来解决动态环境下的传统机器学习效果不好的问题。这也就将封闭静态的传统学习方式转变为开放动态的在线学习模式。

## General Progress
在介绍完在线学习提出的动机之后，接下来将介绍在线学习的基本学习流程。在在线学习中，算法将进行一轮又一轮的学习过程。在第$t$轮的时候，我们将会遇到一个环境给出的问题$x\in\chi$，$\chi$为问题空间，算法给出一个预测的答案$p_t\in D$，$D$为预测空间，然后环境给出正确的答案$y_t$，我们将得到一个损失$l(p_t,y_t)$。总结来说就是如下：
![OL](https://pic3.zhimg.com/v2-4d4727310e1080ce387db91a881fd84a_r.jpg)

上一篇文章介绍完在线学习的基本概念，算是对在线学习问题有了一个基本的引入。本文将介绍FTL算法。

## FTL

对于当前轮的问题，我们不知道最好的解是什么，但是我们可以利用以前的那些轮得出的结果，一个最直观易懂的想法就是我们直接用使得以往的损失和最小的那个解作为我们当前轮做出的预测。这也就是Follow-The-Leader算法的想法来源，这个leader也就是在之前表现最好的那个解，我们follow这个解，认为这个解在当前轮的表现也会很好。用数学表示就是

$$ \begin{gathered} \forall t, \quad \mathbf{w_t}=\underset{\mathbf{w} \in S}{\operatorname{argmin}} \sum_{i=1}^{t-1} f_i(\mathbf{w}) \quad \end{gathered} $$

那么这个直觉上的算法可不可行呢？其实在一些简单问题上还是有一个比较小的Regret Bound的。下面将分析FTL算法在在线二次优化问题(Online Quadratic Optimization)上的Regret Bound。

首先，为了简化Regret，我们证明一个引理。

对任意一个向量$u∈S$,和FTL算法产生的一系列向量\mathbf{w1,w2,w3,...},都有下面的不等式成立

$\operatorname{Regret_T}(\mathbf{u})=\sum_{t=1}^T\left(f_t\left(\mathbf{w_t}\right)-f_t(\mathbf{u})\right) \leq \sum_{t=1}^T\left(f_t\left(\mathbf{w_t}\right)-f_t\left(\mathbf{w_{t+1}}\right)\right)$
